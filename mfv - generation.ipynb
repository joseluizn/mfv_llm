{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from llm_utils import *\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authenticate with the APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=getpass(\"Enter API key: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Role message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_message = \"\"\"You're an artificial intelligence which reacts to vignettes on psychological scales.\n",
    "For instance, if an user asks you: How much do you agree with the following statement? 'I'm a large language model', 1 - Completely disagree, 2, 3, 4 - Neither agree nor disagree, 5, 6, 7 - Completely agree, you should respond with a number between 1 and 7.\n",
    "Your response SHOULD NOT contain the number's accompanying text. So, if you select '7', you should just return '7', instead of '7 - Completely agree'. Similarly, if they ask: Are you a large language model?, 0 - No, 1 - Yes, you should respond with '0' or '1', instead of '0 - No' or '1 - Yes'.\n",
    "DON'T explain your reasoning. I'm ONLY interested in your response to the scale.\n",
    "Make sure that the response falls within the boundaries of the question. For instance: 3 is NOT an acceptable answer to a question that should be answered with a 0 or a 1\n",
    "If an user asks multiple questions, you should respond with a list of numbers, one for each question.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Gemini Pro\": call_gemini,\n",
    "    #\"Llama 2 Chat 70b\": call_llama2,\n",
    "    #\"Claude 2.1\": call_claude2_1,\n",
    "    #\"GPT-4\": call_gpt4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading mfq info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./stimuli/mfq.json\") as f:\n",
    "    mfq = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading MFV prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the stimuli from mfv.json\n",
    "with open(\"./stimuli/mfv.json\") as f:\n",
    "    mfv_info = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading vignettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_vignettes = pd.read_html(mfv_info[\"vignettes_html\"])[0]\n",
    "original_vignettes[\"mfv_code\"] = mfv_info[\"validated_codes\"]\n",
    "\n",
    "# loading vignittes used in portuguese\n",
    "mfv_pt = pd.read_excel(\"stimuli/mfvignettes_pt.xlsx\", sheet_name=0)\n",
    "double_validated_codes = mfv_pt[\"MFV Code\"].to_list()\n",
    "foundations = mfv_pt[\"Foundation\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mfq_stimuli(randomize=False):\n",
    "    global mfq\n",
    "    if randomize is False:\n",
    "        return {\n",
    "            \"p1\": mfq[\"mfq_part1\"],\n",
    "            \"p2\": mfq[\"mfq_part2\"],\n",
    "            \"items1\": mfq[\"part1_item_key\"],\n",
    "            \"items2\": mfq[\"part2_item_key\"],\n",
    "        }\n",
    "    else:\n",
    "        from sklearn.utils import shuffle\n",
    "\n",
    "        # randomize the order of the items\n",
    "        p1, p1items = shuffle(mfq[\"part1_questions\"], mfq[\"part1_item_key\"])\n",
    "        p2, p2items = shuffle(mfq[\"part2_questions\"], mfq[\"part2_item_key\"])\n",
    "        # generate string with p1 with number at beginning of each line\n",
    "        p1 = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(p1)])\n",
    "        p2 = \"\\n\".join([f\"{i+1+len(p1items)}. {q}\" for i, q in enumerate(p2)])\n",
    "        return {\"p1\": p1, \"p2\": p2, \"items1\": p1items, \"items2\": p2items}\n",
    "    \n",
    "def generate_mfv_stimuli(randomize=False):\n",
    "    global mfv_info\n",
    "    pre_mfv = mfv_info[\"pre_mfv\"]\n",
    "    if randomize is False:\n",
    "        numbered_vignettes = \"\\n\".join(\n",
    "            (\n",
    "                (original_vignettes.reset_index().index + 1).astype(str)\n",
    "                + \". \"\n",
    "                + original_vignettes[\"Scenario\"]\n",
    "            ).to_list()\n",
    "        )\n",
    "        mfv_prompt = pre_mfv + \"\\n\\n\" + numbered_vignettes\n",
    "        codes = original_vignettes[\"mfv_code\"].to_list()\n",
    "    else:\n",
    "        df = original_vignettes.sample(frac=1).copy()\n",
    "        numbered_vignettes = \"\\n\".join(\n",
    "            (\n",
    "                (df.reset_index().index + 1).astype(str)\n",
    "                + \". \"\n",
    "                + df[\"Scenario\"]\n",
    "            ).to_list()\n",
    "        )\n",
    "        mfv_prompt = pre_mfv + \"\\n\\n\" + numbered_vignettes\n",
    "        codes = df[\"mfv_code\"].to_list()\n",
    "    return {\n",
    "        \"prompt\": mfv_prompt,\n",
    "        \"mfv_codes\": codes,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation - MfVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_vignettes = pd.read_html(mfv_info[\"vignettes_html\"])[0]\n",
    "original_vignettes[\"mfv_code\"] = mfv_info[\"validated_codes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading vignittes used in portuguese\n",
    "mfv_pt = pd.read_excel(\"stimuli/mfvignettes_pt.xlsx\", sheet_name=0)\n",
    "double_validated_codes = mfv_pt[\"MFV Code\"].to_list()\n",
    "foundations = mfv_pt[\"Foundation\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintaining only validated in brazilian replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintaining only validated in brazilian replication\n",
    "original_vignettes.query(\"mfv_code in @double_validated_codes\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mfq(model, chat_history):\n",
    "    mfq_info = generate_mfq_stimuli(randomize=True)\n",
    "    mfq_part1 = mfq_info[\"p1\"]\n",
    "    mfq_part2 = mfq_info[\"p2\"]\n",
    "    chat_history.append({\"role\": \"user\", \"content\": mfq_part1})\n",
    "    part1_answer = model(chat_history)\n",
    "\n",
    "    chat_history.extend(\n",
    "        [\n",
    "            {\"role\": \"assistant\", \"content\": part1_answer},\n",
    "            {\"role\": \"user\", \"content\": mfq_part2},\n",
    "        ]\n",
    "    )\n",
    "    # if part1 is not none\n",
    "    if part1_answer:\n",
    "        part2_answer = model(chat_history)\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": part2_answer})\n",
    "    else:\n",
    "        part2_answer = None\n",
    "    return (\n",
    "        {\n",
    "            \"part1\": part1_answer,\n",
    "            \"part2\": part2_answer,\n",
    "            \"chat_history\": chat_history,\n",
    "            \"part1_order\": mfq_info[\"items1\"],\n",
    "            \"part2_order\": mfq_info[\"items2\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def run_mfv(model, chat_history):\n",
    "    mfv_info = generate_mfv_stimuli(randomize=True)\n",
    "    mfv_prompt = mfv_info[\"prompt\"]\n",
    "    chat_history.append({\"role\": \"user\", \"content\": mfv_prompt})\n",
    "    mfv_answer = model(chat_history)\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": mfv_answer})\n",
    "    return {\n",
    "        \"mfv\": mfv_answer,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"mfv_codes\": mfv_info[\"mfv_codes\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_mfv_condition(model, agent_name, id, condition):\n",
    "    base_chat = [\n",
    "        {\"role\": \"system\", \"content\": role_message},\n",
    "    ]\n",
    "\n",
    "    chat_log = base_chat.copy()\n",
    "    if condition == \"qv\":\n",
    "        # call mfq\n",
    "        mfq_answer = run_mfq(model, chat_log)\n",
    "\n",
    "        # call mfv\n",
    "        mfv_answer = run_mfv(model, chat_log)\n",
    "    elif condition == \"vq\":\n",
    "        # call mfv\n",
    "        mfv_answer = run_mfv(model, chat_log)\n",
    "\n",
    "        # call mfq\n",
    "        mfq_answer = run_mfq(model, chat_log)\n",
    "    else:\n",
    "        raise ValueError(\"Condition must be 'qv' or 'vq'\")\n",
    "\n",
    "    # append to responses\n",
    "    return {\n",
    "        \"agent\": agent_name,\n",
    "        \"id\": id,\n",
    "        \"condition\": condition,\n",
    "        # \"ideology\": response_ideology,\n",
    "        \"mfq\": mfq_answer,\n",
    "        \"mfv\": mfv_answer,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(n):\n",
    "    results = list()\n",
    "    for i in range(n):\n",
    "        for agent, model in models.items():\n",
    "            for condition in [\"qv\", \"vq\"]:\n",
    "                results.append(generate_mfv_condition(model, agent, i, condition))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = run_experiment(5)\n",
    "\n",
    "with open(\"responses/mfv_test_gemini.json\", \"w\") as f:\n",
    "    json.dump(r, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
